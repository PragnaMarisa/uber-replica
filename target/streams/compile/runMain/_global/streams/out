[0m[[0m[31merror[0m] [0m[0morg.apache.spark.sql.AnalysisException: A schema mismatch detected when writing to the Delta table (Table ID: 0a7d57b3-612f-4d44-a004-994745a7c077).[0m
[0m[[0m[31merror[0m] [0m[0mTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:[0m
[0m[[0m[31merror[0m] [0m[0m'.option("mergeSchema", "true")'.[0m
[0m[[0m[31merror[0m] [0m[0mFor other operations, set the session configuration[0m
[0m[[0m[31merror[0m] [0m[0mspark.databricks.delta.schema.autoMerge.enabled to "true". See the documentation[0m
[0m[[0m[31merror[0m] [0m[0mspecific to the operation for details.[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mTable schema:[0m
[0m[[0m[31merror[0m] [0m[0mroot[0m
[0m[[0m[31merror[0m] [0m[0m-- ride_id: integer (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- passenger_id: integer (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- pickup_location: long (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- ride_status: string (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- timestamp: timestamp (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- ride_date: date (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mData schema:[0m
[0m[[0m[31merror[0m] [0m[0mroot[0m
[0m[[0m[31merror[0m] [0m[0m-- ride_id: integer (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- passenger_id: integer (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- driver_id: integer (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- pickup_location: long (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- ride_status: string (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- timestamp: timestamp (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- ride_date: date (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0m         [0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.delta.MetadataMismatchErrorBuilder.finalizeAndThrow(DeltaErrors.scala:3370)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.delta.schema.ImplicitMetadataOperation.updateMetadata(ImplicitMetadataOperation.scala:140)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.delta.schema.ImplicitMetadataOperation.updateMetadata$(ImplicitMetadataOperation.scala:56)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.delta.commands.WriteIntoDelta.updateMetadata(WriteIntoDelta.scala:76)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.delta.commands.WriteIntoDelta.write(WriteIntoDelta.scala:162)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:105)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1$adapted(WriteIntoDelta.scala:100)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:223)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.delta.commands.WriteIntoDelta.run(WriteIntoDelta.scala:100)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:201)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:304)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.uberreplica.ride.RequestRide$.insertRideRequests(RequestRide.scala:92)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.uberreplica.ride.RequestRide$.main(RequestRide.scala:34)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.uberreplica.ride.RequestRide.main(RequestRide.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.lang.reflect.Method.invoke(Method.java:566)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:144)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:94)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:121)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:187)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:121)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunMainTask$7(Defaults.scala:2012)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1983)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m[[0m[31merror[0m] [0m[0m(Compile / [31mrunMain[0m) org.apache.spark.sql.AnalysisException: A schema mismatch detected when writing to the Delta table (Table ID: 0a7d57b3-612f-4d44-a004-994745a7c077).[0m
[0m[[0m[31merror[0m] [0m[0mTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:[0m
[0m[[0m[31merror[0m] [0m[0m'.option("mergeSchema", "true")'.[0m
[0m[[0m[31merror[0m] [0m[0mFor other operations, set the session configuration[0m
[0m[[0m[31merror[0m] [0m[0mspark.databricks.delta.schema.autoMerge.enabled to "true". See the documentation[0m
[0m[[0m[31merror[0m] [0m[0mspecific to the operation for details.[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mTable schema:[0m
[0m[[0m[31merror[0m] [0m[0mroot[0m
[0m[[0m[31merror[0m] [0m[0m-- ride_id: integer (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- passenger_id: integer (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- pickup_location: long (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- ride_status: string (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- timestamp: timestamp (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- ride_date: date (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mData schema:[0m
[0m[[0m[31merror[0m] [0m[0mroot[0m
[0m[[0m[31merror[0m] [0m[0m-- ride_id: integer (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- passenger_id: integer (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- driver_id: integer (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- pickup_location: long (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- ride_status: string (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- timestamp: timestamp (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m-- ride_date: date (nullable = true)[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0m         [0m
